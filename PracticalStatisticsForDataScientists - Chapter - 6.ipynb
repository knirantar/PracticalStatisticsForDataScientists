{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bd3c9d6",
   "metadata": {},
   "source": [
    "<h2>K Nearest Neighbours</h2><br>\n",
    "1. Find K records that have similar features<br>\n",
    "2. For classification, find out what the majority class is among those similar records and assign that class to the new record.<br>\n",
    "3. For prediction (also called KNN regression), find the average among those similar records, and predict that average for the new record.<br>\n",
    "\n",
    "<b>All predictors must be in numeric form</b>\n",
    "\n",
    "<b>Distance metrics for KNN</b>\n",
    "1. Euclidean distance\n",
    "2. Manhattan distance\n",
    "\n",
    "<b>Standardization(Normalization, z-score)</b>\n",
    "For the procedures like KNN, PCA, Clustering it is essential to perform standardization.\n",
    "\n",
    "<b>Choosing K</b>\n",
    "if K is too low, we may be overfitting.\n",
    "if K is too high, we may oversmooth the data and miss out on KNN’s ability to capture the local structure in the data\n",
    "\n",
    "<b>Bias Variance Tradeoff</b>\n",
    "Variance refers to the modeling error that occurs because of the choice of training data<br>\n",
    "Bias refers to the modeling error that occurs because you have not properly identified the underlying real-world scenario; this error would not disappear if you simply added more training data.\n",
    "<b>Cross Validation</b> approach is used to take care of this tradeoff.\n",
    "\n",
    "<h2>Tree Models(Classification and Regression Trees) CART</h2>\n",
    "A tree model is a set of “if-then-else” rules. \n",
    "\n",
    "<b>The Recusrsive Partitioning Algorithm</b>\n",
    "Decision trees recursively partition the predictor variables to get the best decision based on the new data.\n",
    "1. Initialize A with the entire data set.<br>\n",
    "2. Apply the partitioning algorithm to split A into two subpartitions, A1 and A2<br>\n",
    "3. Repeat step 2 on subpartitions A1and A2<br>\n",
    "4. The algorithm terminates when no further partition can be made that sufficiently improves the homogeneity of the partitions.<br>\n",
    "\n",
    "But for above steps to be successfull we need to find the way to measure homogenity or purity of the partition - can be measured by impurity - Gini impurity and Entropy of Information.\n",
    "\n",
    "<b>Stopping the Tree from Growing</b>\n",
    "A fully grown tree results in completely pure leaves and, hence, 100% accuracy in classifying the data that it is trained on. This accuracy is, of course, illusory we have overfit. There are parameters in algorithm min_samples_split  and min_samples_leaf to restrict tree growth.\n",
    "\n",
    "<b>How Trees are used</b>\n",
    "In predictive modeling main concern is of black box modeling - not knowing what is going on in model. But in case of tree models we can see\n",
    "1. Tree models provide a visual tool for exploring the data, to gain an idea of what variables are important and how they relate to one another. Trees can capture onlinear relationships among predictor variables.\n",
    "2. Tree models provide a set of rules that can be effectively communicated to non specialists, either for implementation or to “sell” a data mining project.\n",
    "    \n",
    "    \n",
    "<h2>Bagging and the Random Forest</h2>\n",
    "<b>Ensamble model</b><br>\n",
    "    1. Develop a predictive model and record the predictions for a given data set.<br>\n",
    "    2. Repeat for multiple models on the same data.<br>\n",
    "    3. For each record to be predicted, take an average (or a weighted average, or a majority vote) of the predictions.<br>\n",
    "    \n",
    "<b>Bagging</b>\n",
    "Bagging stands for <b>Bootstrap Aggregating</b> Bagging is like the basic algorithm for ensembles, except that, instead of fitting the various models to the same data, each new model is fitted to a bootstrap resample\n",
    "<b>Random Forest</b> is just applying bagging to decision trees. \n",
    "\n",
    "\n",
    "<h2>Hyperparameters</h2>Random forest is a black box model but with knobs to fit accurate model, these knobs are called Hyperparameters.<br>\n",
    "<b>min_samples_leaf</b> -> The minimum size for terminal nodes <br>\n",
    "<b>max_leaf_nodes</b> -> The maximum number of nodes in each decision tree.<br>\n",
    "\n",
    "<h2>Boosting</h2>\n",
    "Boosting is a general technique to create an ensemble of models. while bagging can be done with relatively little tuning, boosting requires much greater care in its application.\n",
    "<br>\n",
    "Several variants of the algorithm are commonly used: Adaboost(Adjusts weights), gradient boosting(Optimizes cost function), and stochastic gradient boosting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7471655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644d9acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
