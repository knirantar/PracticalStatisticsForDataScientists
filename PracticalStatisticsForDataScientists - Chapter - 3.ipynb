{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e45d749",
   "metadata": {},
   "source": [
    "1. Statistical Experiments and Significance Testing\n",
    "Design of Experiments\n",
    "\n",
    "Formulate Hypothesis --> Design Experiment --> Collect Data --> Inference/Conclusions\n",
    "\n",
    "2. A/B Testing\n",
    "Treatment Group - Group introduced to new treatment or new procedure etc.<br>\n",
    "Control Group - Group with existing treatment or no change<br>\n",
    "Test Statistic - Metric used to compare treatment and control group<br>\n",
    "<br>\n",
    "\n",
    "3. A/B testing examples\n",
    "Testing two soil treatments to determine which produces better seed germination<br>\n",
    "Testing two therapies to determine which suppresses cancer more effectively<br>\n",
    "Testing two prices to determine which yields more net profit<br>\n",
    "Testing two web headlines to determine which produces more clicks<br>\n",
    "Testing two web ads to determine which generates more conversions<br>\n",
    "\n",
    "4. In A/B test there are two things that can produce different results in two treatments<br>\n",
    "    1. Due to effect of different treatments\n",
    "    2. Due to luck of the draw.\n",
    "    \n",
    "5. Why have a control group in A/B testing?\n",
    "Without the use of control group there is no assurance that all other things are same for the treatment group and difference between prior experience and new treatment is due to the new treatment not by just chance.\n",
    "\n",
    "6. In a standard A/B experiment, you need to decide on one metric ahead of time.Selecting a test statistic after the experiment is conducted opens the door to researcher bias.\n",
    "\n",
    "7. Data scientists are less interested in the question:<br>\n",
    "    \"Is the difference between price A and price B statistically significant?\"<br>\n",
    "    than in the question:<br>\n",
    "    \"Which, out of multiple possible prices, is best?\"<br>\n",
    "    \n",
    "8. <h5>Hypothesis Tests</h5> - Why do we need a hypothesis? Why not just look at the outcome of the experiment and go with whichever treatment does better? The answer lies in the tendency of the human mind to underestimate the scope of natural random behavior. One manifestation of this is the failure to anticipate extreme events, or so-called “black swans”\n",
    "\n",
    "9. <h5>The Null Hypothesis</h5> - a baseline assumption that the treatments are equivalent, and any difference between the groups is due to chance. This baseline assumption is termed the null hypothesis\n",
    "\n",
    "10. <h5>Alternative Hypothesis</h5> - Alternative hypothesis is offsetting to null hypothesis. Together, the null and alternative hypotheses must account for all possibilities.\n",
    "\n",
    "11. <h5>One way Vs Two way Hypothesis Tests</h5>\n",
    "<h6> One Way hypothesis test </h6> - if we are testing A and B. B is a new treatment and we need definitive proof to change from A to B. So it may be that we get fooled to accept B as good and reject null hypothesis. But we don't care if we get wrong results and reject the alternative hypothesis and stick with B then we use <strong>One way hypothesis test</strong>\n",
    "\n",
    "<h6>Two way Hypothesis Test</h6> - If you want a hypothesis test to protect you from being fooled by chance in either\n",
    "direction. In such a case, you use a two-way (or two-tail) hypothesis\n",
    "\n",
    "12. Resampling - Resampling in statistics means to repeatedly sample values from observed data\n",
    "    1. Bootstrap\n",
    "    2. Permutation Tests<br>\n",
    "        a. Combine the results from the different groups into a single data set\n",
    "        b. Shuffle the combined data and then randomly draw (without repla.) a resample of the same size as group A \n",
    "        c. From the remaining data, randomly draw (without replacement) a resample of the same size as group B\n",
    "        d. Whatever statistic or estimate was calculated for the original samples - calculate it now - this is one permutation\n",
    "        e. Repeat the process R times\n",
    "        f. If the observed difference lies well within the set of permuted differences, then we have not proven anything—the observed difference is within the range of what chance might produce. if the observed difference lies outside most of the permutation distribution, then we conclude that chance is not responsible, the difference is statistically significant.\n",
    "        \n",
    "13. Types of Permutation Tests\n",
    "    1. Exhaustive Permutation Tests - shuffling the sample so that all possible permuatations are tested - only possible for small sample sizes - also called exact test\n",
    "    2. Bootstrap Permuatation Tests - it involves shuffling with replacement - same as bootstrap sampling - but in permutation test\n",
    "    \n",
    "14. p-value -  This is the frequency with which the chancemodel produces a result more extreme than the observed result. We can estimate a pvalue from our permutation test by taking the proportion of times that the permutation test produces a difference equal to or greater than the observed difference\n",
    "\n",
    "15. alpha - a threshold is specified in advance, as in “more extreme than 5% of the chance (null hypothesis) results”; this threshold is known as alpha.\n",
    "\n",
    "16. Here is what we would like p-value to tell us:\n",
    "    <strong>The probability that the result is due to chance</strong><br>\n",
    "    But here’s what the p-value actually represents:\n",
    "    <strong>The probability that, given a chance model, results as extreme as the observed results could occur </strong>\n",
    "    \n",
    "17. type I error -> A Type 1 error, in which you mistakenly conclude an effect is real, when it is really just due to chance\n",
    "\n",
    "18. A Type 2 error, in which you mistakenly conclude that an effect is not real (i.e., due to chance), when it actually is real\n",
    "\n",
    "19. t-Tests - named after Student's t distribution - All significance tests require that you specify a test statistic to measure the effect - it was not possible in old days to reshuffle and resample the data thousands of times - so they approximated the permutation distribution as t-Distribution\n",
    "\n",
    "20. Multiple Testing - If you run significance tests multiple times with multiple perspectives you will get at least one variable that will be significant and introduce type I error - in statistics if you torture the data long enough it will confess.\n",
    "To avoid the false values in significance tests - we use statistical adjustments such as dividing the alpha according to the number of tests. This reduces the alpha and significance becomes stringent.\n",
    "\n",
    "21. Degrees of Freedom - The concept is applied to statistics calculated from sample data. If you know the mean for a sample of 10 values, there are 9 degrees of freedom (once you know 9 of the sample values, the 10th can be calculated and is not free to vary) - In data science degrees of freedom concept is used in regression to avoid dummy variable trap\n",
    "\n",
    "22. ANOVA - The statistical procedure that tests for a statistically significant difference among the groups is called analysis of variance, or ANOVA. If there are multiple samples and we need to compare means , pairwise comparison will result in multiple testing causing flawed results. For this we use ANOVA. \n",
    "<p>Similar to permutation test - collect all data - reshuffle - assign to each group through randomization - calculate their means - and variance between means - repeat for 1000 times - calculate the proportion of times variance exceeds the observed variance.(p-value)</p>\n",
    "\n",
    "23. Two Way ANOVA - In One Way Anova We have only one varying metric - in two way ANOVA we can have two metric that are varying for each subject.\n",
    "\n",
    "24. Chi-Square Test - The chi-square test is used with count data to test how well it fits some expected distribution. most common use  is with r×c contingency tables.\n",
    " Pearson residual = R = (Observed−Expected)/sqrt(Expected)\n",
    " Observed - actual result after testing\n",
    " Expected - all subjects have same result with testing\n",
    " The chi-square statistic is defined as the sum of the squared Pearson residuals\n",
    " \n",
    "25. Multi arm bandit Algorithm - allows explicit optimization and more rapid decision making than the traditional statistical approach to designing experiments. Traditional A/B test have very less flexibility - need to make decisions on basis of small samples and inconclusive results.\n",
    "In multi arm bandit algorithm - allow you to test multiple treatments at once and reach conclusions faster than traditional statistical designs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23739f07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
