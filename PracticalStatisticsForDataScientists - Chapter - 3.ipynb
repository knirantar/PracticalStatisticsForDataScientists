{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8133dc",
   "metadata": {},
   "source": [
    "1. Statistical Experiments and Significance Testing\n",
    "Design of Experiments\n",
    "\n",
    "Formulate Hypothesis --> Design Experiment --> Collect Data --> Inference/Conclusions\n",
    "\n",
    "2. A/B Testing\n",
    "Treatment Group - Group introduced to new treatment or new procedure etc.<br>\n",
    "Control Group - Group with existing treatment or no change<br>\n",
    "Test Statistic - Metric used to compare treatment and control group<br>\n",
    "<br>\n",
    "\n",
    "3. A/B testing examples\n",
    "Testing two soil treatments to determine which produces better seed germination<br>\n",
    "Testing two therapies to determine which suppresses cancer more effectively<br>\n",
    "Testing two prices to determine which yields more net profit<br>\n",
    "Testing two web headlines to determine which produces more clicks<br>\n",
    "Testing two web ads to determine which generates more conversions<br>\n",
    "\n",
    "4. In A/B test there are two things that can produce different results in two treatments<br>\n",
    "    1. Due to effect of different treatments\n",
    "    2. Due to luck of the draw.\n",
    "    \n",
    "5. Why have a control group in A/B testing?\n",
    "Without the use of control group there is no assurance that all other things are same for the treatment group and difference between prior experience and new treatment is due to the new treatment not by just chance.\n",
    "\n",
    "6. In a standard A/B experiment, you need to decide on one metric ahead of time.Selecting a test statistic after the experiment is conducted opens the door to researcher bias.\n",
    "\n",
    "7. Data scientists are less interested in the question:<br>\n",
    "    \"Is the difference between price A and price B statistically significant?\"<br>\n",
    "    than in the question:<br>\n",
    "    \"Which, out of multiple possible prices, is best?\"<br>\n",
    "    \n",
    "8. <h5>Hypothesis Tests</h5> - Why do we need a hypothesis? Why not just look at the outcome of the experiment and go with whichever treatment does better? The answer lies in the tendency of the human mind to underestimate the scope of natural random behavior. One manifestation of this is the failure to anticipate extreme events, or so-called “black swans”\n",
    "\n",
    "9. <h5>The Null Hypothesis</h5> - a baseline assumption that the treatments are equivalent, and any difference between the groups is due to chance. This baseline assumption is termed the null hypothesis\n",
    "\n",
    "10. <h5>Alternative Hypothesis</h5> - Alternative hypothesis is offsetting to null hypothesis. Together, the null and alternative hypotheses must account for all possibilities.\n",
    "\n",
    "11. <h5>One way Vs Two way Hypothesis Tests</h5>\n",
    "<h6> One Way hypothesis test </h6> - if we are testing A and B. B is a new treatment and we need definitive proof to change from A to B. So it may be that we get fooled to accept B as good and reject null hypothesis. But we don't care if we get wrong results and reject the alternative hypothesis and stick with B then we use <strong>One way hypothesis test</strong>\n",
    "\n",
    "<h6>Two way Hypothesis Test</h6> - If you want a hypothesis test to protect you from being fooled by chance in either\n",
    "direction. In such a case, you use a two-way (or two-tail) hypothesis\n",
    "\n",
    "12. Resampling - Resampling in statistics means to repeatedly sample values from observed data\n",
    "    1. Bootstrap\n",
    "    2. Permutation Tests<br>\n",
    "        a. Combine the results from the different groups into a single data set\n",
    "        b. Shuffle the combined data and then randomly draw (without repla.) a resample of the same size as group A \n",
    "        c. From the remaining data, randomly draw (without replacement) a resample of the same size as group B\n",
    "        d. Whatever statistic or estimate was calculated for the original samples - calculate it now - this is one permutation\n",
    "        e. Repeat the process R times\n",
    "        f. If the observed difference lies well within the set of permuted differences, then we have not proven anything—the observed difference is within the range of what chance might produce. if the observed difference lies outside most of the permutation distribution, then we conclude that chance is not responsible, the difference is statistically significant.\n",
    "        \n",
    "13. Types of Permutation Tests\n",
    "    1. Exhaustive Permutation Tests - shuffling the sample so that all possible permuatations are tested - only possible for small sample sizes - also called exact test\n",
    "    2. Bootstrap Permuatation Tests - it involves shuffling with replacement - same as bootstrap sampling - but in permutation test\n",
    "    \n",
    "14. p-value -  This is the frequency with which the chancemodel produces a result more extreme than the observed result. We can estimate a pvalue from our permutation test by taking the proportion of times that the permutation test produces a difference equal to or greater than the observed difference\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f7a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
