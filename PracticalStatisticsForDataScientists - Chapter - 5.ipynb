{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c32e7eb7",
   "metadata": {},
   "source": [
    "<h1>Classification</h1>\n",
    "In case of Classification there can be binary or multiclass classification. Predicted variable can simply return the class which is predicted or it can return the probabilty of each class present which is more likely to occur.\n",
    "\n",
    "<h2>Naive Bayes</h2><br>\n",
    "1. Conditional Probability - The probability of observing some event (say, X = i) given some other event (say,Y = i), written as P(Xi|Yi).\n",
    "<br>\n",
    "2. Posterior Probability - The probability of an outcome after the predictor information has been incorporated \n",
    "\n",
    "<b>Exact Bayesian Classification(Impractical)</b>\n",
    "<br>\n",
    "1. For each record find all the other records with the same predictor profile.<br>\n",
    "2. Determine what classes those records belong to<br>\n",
    "3. Assign that class to the new record.<br>\n",
    "Provided that all predictors are categorical.\n",
    "\n",
    "<b>Baye's Theorem</b>\n",
    "In case of dependent events Conditional Probability is P(A|B) - Probability of A given B = P(A int. B)/ P(B)\n",
    "based on the conditional probability formula we can derive bayes theorem by equating P(A int. B) = P(B int. A)\n",
    "\n",
    "Bayes theorem = P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "<b>Naive Bayes Classifier</b>\n",
    "P(y|x1,x2,x3...) = P(x1|y) * P(x2|y) * P(x3|y)....* P(y) / P(x1)* P(X2) * P(x3)...\n",
    "\n",
    "<b> Naive Bayes works with categorical predictors only if we want to use naive bayes with continuous variables we need to convert variable to categories using bins or use normal distribution to estimate conditional probabilities.</b>\n",
    "\n",
    "\n",
    "<h2>Discriminant Analysis</h2>The most commonly used technique is linear discriminant analysis, or LDA. Less used. \n",
    "<b>LDA</b>\n",
    "LDA assumes that predictor variables are continuous and normally distributed. But even if they are far from normality LDA works quite well. LDA tries to distinguish the data into two groups. LDA does work with more than two predictors. LDA also works with continuous as well as categorical predictors.\n",
    "\n",
    "<h2>Logistic Regression</h2>\n",
    "Logistic regression is analogous to multiple linear regression just output is binary variable.\n",
    "<b>Logit</b><br>\n",
    "The function that maps class membership probability to a range from ± ∞. Its an 'S' shaped function and its fits the data to get binary outcome. Curve of the function goes from 0 to 1.<br>\n",
    "Just like linear regression logistic regression can work with continuous data and multiple predictor variables.\n",
    "In linear regression we fit the line with the help of least squares method - and similarly in logistic regression we fit the 'S' shaped function that gives us maximum likelihood. (MLE) Maximum Likelihood Estimation.\n",
    "\n",
    "<h2>Assessing Classification Models</h2>It is common in predictive modeling to train a number of different models, apply each to a holdout sample, and assess their performance\n",
    "<b>Accuracy</b>\n",
    "acc = (True Positive + True Negative)/sample size\n",
    "<br>\n",
    "<b>Confusion Matrix</b>\n",
    "The predicted outcomes are columns and the true outcomes are the rows. The diago‐\n",
    "nal elements of the matrix show the number of correct predictions, and the offdiagonal elements show the number of incorrect predictions.\n",
    "<br>\n",
    "<b>Precision</b> = True Positive/(True Positive + False Positive)\n",
    "<br>\n",
    "<b>Recall</b> = True Positive/(True Positive + False Negative)\n",
    "<br>\n",
    "<b>Specificity</b> = True Negative/ (True Negative + False Positive)\n",
    "<br>\n",
    "\n",
    "<h2>ROC Curve</h2>\n",
    "The ROC curve plots recall (sensitivity) on the y-axis against specificity on the x-axis.\n",
    "Recall -> How many positives are correctly predicted out of total positives<br>\n",
    "Specificity -> How many negatives are correctly predicted out of total negatives<br>\n",
    "ROC -> Specificity VS. Recall graph<br>\n",
    "For good classifier it must touch the upper left corner - stating that more positive values are correctly classified rather than misclassifiying negatives as positive\n",
    "\n",
    "<h2>AUC</h2>\n",
    "Area Under The curve - is the aread under the ROC curve - larger the are more effective is the classifier.\n",
    "\n",
    " \n",
    "\n",
    "<h2>Rare Class Problem</h2>\n",
    "Sometimes we may have data with one class very less data points that is rare and another class with lots of data points.\n",
    "<b>Strategies for Imbalanced Data</b><br>\n",
    "1. Undersampling - reduce the dominant class data. One disadvantage of undersampling is it throws away data and do not use all information at hand.\n",
    "\n",
    "2. Oversampling - increase the rarer class rows with drawing additional rows with replacement(Bootstrap)\n",
    "\n",
    "3. Data Generation - data generation is creating new rows that similar but not identical to existing records \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7eca18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
